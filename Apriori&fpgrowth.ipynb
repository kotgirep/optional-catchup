{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apriori&fpgrowth.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yez7ikiEDWPU"
      },
      "source": [
        "CMPE-255: Write a colab to demonstrate frequent pattern mining apriori and fpgrowth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CWKHJVssWyp"
      },
      "source": [
        "frequent pattern mining with apriori Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pbz6fL-DR1j"
      },
      "source": [
        "The Apriori Algorithm is commonly used for frequent pattern mining, and FP-growth is an upgraded version of it (AKA Association Rule Mining). It's an analytical technique for identifying common patterns or correlations in data sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdOlKpQ0DmNP"
      },
      "source": [
        "Han proposed the FP-Growth Algorithm, which is an efficient and scalable method for mining the entire set of frequent patterns by pattern fragment growth, which uses an extended prefix-tree structure for storing compressed and crucial information about frequent patterns called the frequent-pattern tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVW6qZBXEExb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr8K0E7kDp_q"
      },
      "source": [
        "Install apyori module for Apriori Pattern"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBtG0Ry8sBNi",
        "outputId": "a2bbfd9d-74a1-47ae-ebe0-6aede0556192"
      },
      "source": [
        "!pip install apyori  \n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from apyori import apriori\n",
        "import math\n",
        "import itertools\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apyori\n",
            "  Downloading apyori-1.1.2.tar.gz (8.6 kB)\n",
            "Building wheels for collected packages: apyori\n",
            "  Building wheel for apyori (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apyori: filename=apyori-1.1.2-py3-none-any.whl size=5974 sha256=e049ff5937c12f34f4a34cca2bedc97e437aa07aaad422ba2da2eab7f437a101\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/f6/e1/57973c631d27efd1a2f375bd6a83b2a616c4021f24aab84080\n",
            "Successfully built apyori\n",
            "Installing collected packages: apyori\n",
            "Successfully installed apyori-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDilkEDVsco6"
      },
      "source": [
        "customer_data = pd.read_csv('/content/Mall_Customers.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMeFSO7otMmR"
      },
      "source": [
        "There is no header row in this dataset. The pd.read csv function, on the other hand, treats the first row as a header by default. To solve this issue, use the header=None option in the pd.read csv function, as illustrated below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "VuPGmjRrsqqV",
        "outputId": "824165f8-9af4-4214-d85a-ebb8f0d46dd4"
      },
      "source": [
        "customer_data = pd.read_csv('/content/Mall_Customers.csv', header=None)\n",
        "customer_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CustomerID</td>\n",
              "      <td>Genre</td>\n",
              "      <td>Age</td>\n",
              "      <td>Annual Income (k$)</td>\n",
              "      <td>Spending Score (1-100)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0001</td>\n",
              "      <td>Male</td>\n",
              "      <td>19</td>\n",
              "      <td>15</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0002</td>\n",
              "      <td>Male</td>\n",
              "      <td>21</td>\n",
              "      <td>15</td>\n",
              "      <td>81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0003</td>\n",
              "      <td>Female</td>\n",
              "      <td>20</td>\n",
              "      <td>16</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0004</td>\n",
              "      <td>Female</td>\n",
              "      <td>23</td>\n",
              "      <td>16</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            0       1    2                   3                       4\n",
              "0  CustomerID   Genre  Age  Annual Income (k$)  Spending Score (1-100)\n",
              "1        0001    Male   19                  15                      39\n",
              "2        0002    Male   21                  15                      81\n",
              "3        0003  Female   20                  16                       6\n",
              "4        0004  Female   23                  16                      77"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xytFaDItT-C"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R9dnP_8tBuf"
      },
      "source": [
        "customers = []\n",
        "for j in range(0, 200):\n",
        "    customers.append([str(customer_data.values[j,k]) for k in range(0, 4)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7wPlxsVuFYm"
      },
      "source": [
        "The dataset will next be subjected to the Apriori algorithm. To do so, we can use the apriori class from the apyori package, which we loaded.\n",
        "\n",
        "To function, the apriori class requires some parameter values. The first parameter is the list of lists from which rules should be extracted. The min support argument is the second parameter. This parameter is used to select items that have a support value larger than the parameter's value. The min confidence parameter then filters out any rules with a confidence level higher than the confidence threshold set by the parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2xO0526tyh-"
      },
      "source": [
        "apriori_rules = apriori(customers, min_support=0.0045, min_confidence=0.2, min_lift=2, min_length=1)\n",
        "association_results = list(apriori_rules)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rty2Tv0_kf9y"
      },
      "source": [
        "Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbxCSr59d9Gq"
      },
      "source": [
        "import sys\n",
        "\n",
        "from itertools import chain, combinations\n",
        "from collections import defaultdict\n",
        "from optparse import OptionParser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9gIcz3RkjpO"
      },
      "source": [
        "Write function to find subset of given array which will return non empty subsets of an array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1-d972yeBAM"
      },
      "source": [
        "def array_subsets(arr):\n",
        "    \"\"\" Returns non empty subsets of arr\"\"\"\n",
        "    return chain(*[combinations(arr, i + 1) for i, a in enumerate(arr)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF2z6_v4l2Of"
      },
      "source": [
        "Write a function to return minimum items in subset of array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA15OmwfeEmV"
      },
      "source": [
        "def return_min_items(allitems, transactionList, minSupport, freqSet):\n",
        "        \"\"\"calculates the support for items in the itemSet and returns a subset\n",
        "       of the itemSet each of whose elements satisfies the minimum support\"\"\"\n",
        "        _itemSet = set()\n",
        "        localSet = defaultdict(int)\n",
        "        for itms in allitems:\n",
        "                for transaction in transactionList:\n",
        "                        if item.issubset(transaction):\n",
        "                                freqSet[itms] += 1\n",
        "                                localSet[itms] += 1\n",
        "\n",
        "        for itms, count in localSet.allitems():\n",
        "                support = float(count)/len(transactionList)\n",
        "\n",
        "                if support >= minSupport:\n",
        "                        _itemSet.add(itms)\n",
        "\n",
        "        return _itemSet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZElLV-IBDkz"
      },
      "source": [
        "Write a function to join itemsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DihLaZtMeIYz"
      },
      "source": [
        "def joinSet(itemSet, length):\n",
        "        \"\"\"Join a set with itself and returns the n-element itemsets\"\"\"\n",
        "        return set([k.union(j) for k in itemSet for j in itemSet if len(k.union(j)) == length])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW4CbhVQeLi5"
      },
      "source": [
        "def gettransaction(data_iterator):\n",
        "    transactionList = list()\n",
        "    allitems = set()\n",
        "    for all_rcrd in data_iterator:\n",
        "        get_trans = frozenset(all_rcrd)\n",
        "        transactionList.append(get_trans)\n",
        "        for item in transaction:\n",
        "            allitems.add(frozenset([item]))              # Generate 1-itemSets\n",
        "    return allitems, transactionList"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7b5cQxqeRAT"
      },
      "source": [
        "def runApriori(data_iter, minSupport, minConfidence):\n",
        "    \"\"\"\n",
        "    run the apriori algorithm. data_iter is a record iterator\n",
        "    Return both:\n",
        "     - items (tuple, support)\n",
        "     - rules ((pretuple, posttuple), confidence)\n",
        "    \"\"\"\n",
        "    itemSet, transactionList = getItemSetTransactionList(data_iter)\n",
        "\n",
        "    freqSet = defaultdict(int)\n",
        "    largeSet = dict()\n",
        "    # Global dictionary which stores (key=n-itemSets,value=support)\n",
        "    # which satisfy minSupport\n",
        "\n",
        "    assocRules = dict()\n",
        "    # Dictionary which stores Association Rules\n",
        "\n",
        "    oneCSet = returnItemsWithMinSupport(allitems,\n",
        "                                        transactionList,\n",
        "                                        minSupport,\n",
        "                                        freqSet)\n",
        "\n",
        "    currentLSet = oneCSet\n",
        "    k = 2\n",
        "    while(currentLSet != set([])):\n",
        "        largeSet[k-1] = currentLSet\n",
        "        currentLSet = joinSet(currentLSet, k)\n",
        "        currentCSet = returnItemsWithMinSupport(currentLSet,\n",
        "                                                transactionList,\n",
        "                                                minSupport,\n",
        "                                                freqSet)\n",
        "        currentLSet = currentCSet\n",
        "        k = k + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjf53gStfO62"
      },
      "source": [
        "FpGrowth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsxmMWSrBcQT"
      },
      "source": [
        "Install the fogrowth module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSKrpZEnfMw6",
        "outputId": "d1e576b7-0425-4a34-bc03-6c4762671a46"
      },
      "source": [
        "pip install fpgrowth_py\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fpgrowth_py\n",
            "  Downloading fpgrowth_py-1.0.0-py3-none-any.whl (5.6 kB)\n",
            "Installing collected packages: fpgrowth-py\n",
            "Successfully installed fpgrowth-py-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dmWvioYBf3N"
      },
      "source": [
        "Install the imap module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJywt72ggUYW",
        "outputId": "be80adf0-aefb-4a65-f604-1c2dde7c2eb7"
      },
      "source": [
        "!pip install imap"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imap\n",
            "  Downloading imap-1.0.0.tar.gz (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imap) (1.19.5)\n",
            "Collecting scanpy\n",
            "  Downloading scanpy-1.8.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 22.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from imap) (1.10.0+cu111)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from imap) (1.1.5)\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[K     |████████████████████████████████| 646 kB 47.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->imap) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->imap) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->imap) (1.15.0)\n",
            "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (0.51.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (21.3)\n",
            "Collecting umap-learn>=0.3.10\n",
            "  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (3.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (4.62.3)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (0.5.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (0.11.2)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (2.6.3)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (5.5.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (1.1.0)\n",
            "Collecting sinfo\n",
            "  Downloading sinfo-0.3.4.tar.gz (24 kB)\n",
            "Collecting anndata>=0.7.4\n",
            "  Downloading anndata-0.7.8-py3-none-any.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 9.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata>=0.7 in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (4.8.2)\n",
            "Requirement already satisfied: tables in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (3.4.4)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (0.10.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (1.0.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.2 in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (3.2.2)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from scanpy->imap) (1.4.1)\n",
            "Requirement already satisfied: xlrd<2.0 in /usr/local/lib/python3.7/dist-packages (from anndata>=0.7.4->scanpy->imap) (1.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.10.0->scanpy->imap) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy->imap) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=0.7->scanpy->imap) (3.10.0.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy->imap) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy->imap) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1.2->scanpy->imap) (3.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy->imap) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.41.0->scanpy->imap) (0.34.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->scanpy->imap) (3.0.0)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.5.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 49.8 MB/s \n",
            "\u001b[?25hCollecting stdlib_list\n",
            "  Downloading stdlib_list-0.8.0-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.7/dist-packages (from tables->scanpy->imap) (2.7.3)\n",
            "Building wheels for collected packages: imap, annoy, umap-learn, pynndescent, sinfo\n",
            "  Building wheel for imap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imap: filename=imap-1.0.0-py3-none-any.whl size=10247 sha256=1a805a1d36ae7cbacd659db7e81b6a24afd32e48c9b9e7e9458a82f37c17a832\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/06/58/24adf464951e7db0a10cb415d7503d7113d225f3c871cf4f93\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391674 sha256=d509082128f59ec098c15d220d1673b6fc51213d5b6c5d890718704ca145e239\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82709 sha256=ff65c3d29787049ee5e53dc10022ba5f37cc1225f35d0b83eab1039c83965f20\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.5-py3-none-any.whl size=52603 sha256=b4e2aaeb2ec98667dc49f6361dd5140b5ea5c0487cb6bbd52feb3f70709918a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/e9/33/04db1436df0757c42fda8ea6796d7a8586e23c85fac355f476\n",
            "  Building wheel for sinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sinfo: filename=sinfo-0.3.4-py3-none-any.whl size=7899 sha256=fb0249b0eea17ff784d474aaaa37b04f74ac7dc3acb84037fa3c52b1ac36e73f\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/ca/56/344d532fe53e855ccd6549795d370588ab8123907eecf4cf30\n",
            "Successfully built imap annoy umap-learn pynndescent sinfo\n",
            "Installing collected packages: stdlib-list, pynndescent, umap-learn, sinfo, anndata, scanpy, annoy, imap\n",
            "Successfully installed anndata-0.7.8 annoy-1.17.0 imap-1.0.0 pynndescent-0.5.5 scanpy-1.8.2 sinfo-0.3.4 stdlib-list-0.8.0 umap-learn-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zFxI8AGBlDy"
      },
      "source": [
        "Add all the required imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sYFjfiXgy9G"
      },
      "source": [
        "from builtins import map"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B428696BBqb7"
      },
      "source": [
        "Write function to find freq dataset items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR1ChPIAB6NF"
      },
      "source": [
        " Load the passed-in transactions and count the support that individual items have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kioEddSUiDq0"
      },
      "source": [
        "def find_freq(transactions, minimum_support, include_support=False):\n",
        "    \n",
        "    allitems = defaultdict(lambda: 0) # mapping from items to their supports\n",
        "\n",
        "    \n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            allitems[item] += 1\n",
        "\n",
        "    # Remove infrequent items from the item support dictionary.\n",
        "    allitems = dict((item, support) for item, support in allitems.iteritems()\n",
        "        if support >= minimum_support)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1MKDQvCBJf"
      },
      "source": [
        "Build our FP-tree. Before any transactions can be added to the tree, they\n",
        "must be stripped of infrequent items and their surviving items must be\n",
        "sorted in decreasing order of frequency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uicyJ_7yCQLF"
      },
      "source": [
        " if there is already a node in  tree for the current transaction item; reuse it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwPyT4a5i7Vl"
      },
      "source": [
        "def find_frequent_itemsets(transactions, minimum_support, include_support=False):\n",
        "    \n",
        "    items = defaultdict(lambda: 0) # mapping from items to their supports\n",
        "\n",
        "   \n",
        "    for transaction in transactions:\n",
        "        for item in transaction:\n",
        "            items[item] += 1\n",
        "\n",
        "    # Remove infrequent items from the item support dictionary.\n",
        "    items = dict((item, support) for item, support in items.iteritems()\n",
        "        if support >= minimum_support)\n",
        "\n",
        "    \n",
        "    def clean_transaction(transaction):\n",
        "        transaction = filter(lambda v: v in items, transaction)\n",
        "        transaction.sort(key=lambda v: items[v], reverse=True)\n",
        "        return transaction\n",
        "\n",
        "    master = FPTree()\n",
        "    for transaction in imap(clean_transaction, transactions):\n",
        "        master.add(transaction)\n",
        "\n",
        "    def find_with_suffix(tree, suffix):\n",
        "        for item, nodes in tree.items():\n",
        "            support = sum(n.count for n in nodes)\n",
        "            if support >= minimum_support and item not in suffix:\n",
        "                # New winner!\n",
        "                found_set = [item] + suffix\n",
        "                yield (found_set, support) if include_support else found_set\n",
        "\n",
        "                \n",
        "                cond_tree = conditional_tree_from_paths(tree.prefix_paths(item))\n",
        "                for s in find_with_suffix(cond_tree, found_set):\n",
        "                    yield s # pass along the good news to our caller\n",
        "\n",
        "    # Search for frequent itemsets, and yield the results we find.\n",
        "    for itemset in find_with_suffix(master, []):\n",
        "        yield itemset\n",
        "\n",
        "class FPTree(object):\n",
        "    \n",
        "\n",
        "    Route = namedtuple('Route', 'head tail')\n",
        "\n",
        "    def __init__(self):\n",
        "        # The root node of the tree.\n",
        "        self._root = FPNode(self, None, None)\n",
        "\n",
        "        # A dictionary mapping items to the head and tail of a path of\n",
        "        # \"neighbors\" that will hit every node containing that item.\n",
        "        self._routes = {}\n",
        "\n",
        "    @property\n",
        "    def root(self):\n",
        "        \"\"\"The root node of the tree.\"\"\"\n",
        "        return self._root\n",
        "\n",
        "    def add(self, transaction):\n",
        "        \"\"\"Add a transaction to the tree.\"\"\"\n",
        "        point = self._root\n",
        "\n",
        "        for item in transaction:\n",
        "            next_point = point.search(item)\n",
        "            if next_point:\n",
        "               \n",
        "                next_point.increment()\n",
        "            else:\n",
        "                # Create a new point and add it as a child of the point we're\n",
        "                # currently looking at.\n",
        "                next_point = FPNode(self, item)\n",
        "                point.add(next_point)\n",
        "\n",
        "                # Update the route of nodes that contain this item to include\n",
        "                # our new node.\n",
        "                self._update_route(next_point)\n",
        "\n",
        "            point = next_point"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlhIXrFwCYA9"
      },
      "source": [
        "Write a function to update the route"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ83CZXQi86y"
      },
      "source": [
        "def rout_new(self, new_row):\n",
        "        \n",
        "        assert self is new_row.tree\n",
        "\n",
        "        try:\n",
        "            route = self._routes[new_row.item]\n",
        "            route[1].neighbor = new_row # route[1] is the tail\n",
        "            self._routes[new_row.item] = self.Route(route[0], new_row)\n",
        "        except KeyError:\n",
        "            # First node for this item; start a new route.\n",
        "            self._routes[new_row.item] = self.Route(new_row, new_row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpJZjV5BjEsj"
      },
      "source": [
        "def items(self):\n",
        "        \n",
        "        for allitm in self._routes.iterkeys():\n",
        "            yield (allitm, self.nodes(allitm))\n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAtfimPtjIlP"
      },
      "source": [
        " def nodes(self, allitm):\n",
        "        \n",
        "\n",
        "        try:\n",
        "            node = self._routes[allitm][0]\n",
        "        except KeyError:\n",
        "            return\n",
        "\n",
        "        while node:\n",
        "            yield node\n",
        "            node = node.neighbor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP6MblBsjMtE"
      },
      "source": [
        "  def prefix_paths(self, allitm):\n",
        "        \"\"\"Generate the prefix paths that end with the given item.\"\"\"\n",
        "\n",
        "        def collect_path(node):\n",
        "            path = []\n",
        "            while node and not node.root:\n",
        "                path.append(node)\n",
        "                node = node.parent\n",
        "            path.reverse()\n",
        "            return path\n",
        "\n",
        "        return (collect_path(node) for node in self.nodes(allitm))\n",
        "\n",
        "  def inspect(self):\n",
        "      print ('Tree:')\n",
        "      self.root.inspect(1)\n",
        "\n",
        "      print\n",
        "      print ('Routes:')\n",
        "      for allitm, nodes in self.items():\n",
        "          print ('  %r' % allitm)\n",
        "         \n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFbgEjAGjQTc"
      },
      "source": [
        "def conditional_tree_from_paths(paths):\n",
        "    \"\"\"Build a conditional FP-tree from the given prefix paths.\"\"\"\n",
        "    tree = FPTree()\n",
        "    condition_item = None\n",
        "    items = set()\n",
        "\n",
        "    # Import the nodes in the paths into the new tree. Only the counts of the\n",
        "    # leaf notes matter; the remaining counts will be reconstructed from the\n",
        "    # leaf counts.\n",
        "    for path in paths:\n",
        "        if condition_item is None:\n",
        "            condition_item = path[-1].item\n",
        "\n",
        "        point = tree.root\n",
        "        for node in path:\n",
        "            next_point = point.search(node.item)\n",
        "            if not next_point:\n",
        "                # Add a new node to the tree.\n",
        "                items.add(node.item)\n",
        "                count = node.count if node.item == condition_item else 0\n",
        "                next_point = FPNode(tree, node.item, count)\n",
        "                point.add(next_point)\n",
        "                tree._update_route(next_point)\n",
        "            point = next_point\n",
        "\n",
        "    assert condition_item is not None\n",
        "\n",
        "    # Calculate the counts of the non-leaf nodes.\n",
        "    for path in tree.prefix_paths(condition_item):\n",
        "        count = path[-1].count\n",
        "        for node in reversed(path[:-1]):\n",
        "            node._count += count\n",
        "\n",
        "    return tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQEfZc8lkaUr"
      },
      "source": [
        "class FPNode(object):\n",
        "    \"\"\"A node in an FP tree.\"\"\"\n",
        "\n",
        "    def __init__(self, tree, item, count=1):\n",
        "        self._tree = tree\n",
        "        self._item = item\n",
        "        self._count = count\n",
        "        self._parent = None\n",
        "        self._children = {}\n",
        "        self._neighbor = None\n",
        "\n",
        "    def add(self, child):\n",
        "        \"\"\"Add the given FPNode `child` as a child of this node.\"\"\"\n",
        "\n",
        "        if not isinstance(child, FPNode):\n",
        "            raise TypeError(\"Can only add other FPNodes as children\")\n",
        "\n",
        "        if not child.item in self._children:\n",
        "            self._children[child.item] = child\n",
        "            child.parent = self\n",
        "\n",
        "    def search(self, item):\n",
        "        \"\"\"\n",
        "        Check whether this node contains a child node for the given item.\n",
        "        If so, that node is returned; otherwise, `None` is returned.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return self._children[item]\n",
        "        except KeyError:\n",
        "            return None\n",
        "\n",
        "    def __contains__(self, item):\n",
        "        return item in self._children\n",
        "\n",
        "    @property\n",
        "    def tree(self):\n",
        "        \"\"\"The tree in which this node appears.\"\"\"\n",
        "        return self._tree\n",
        "\n",
        "    @property\n",
        "    def item(self):\n",
        "        \"\"\"The item contained in this node.\"\"\"\n",
        "        return self._item\n",
        "\n",
        "    @property\n",
        "    def count(self):\n",
        "        \"\"\"The count associated with this node's item.\"\"\"\n",
        "        return self._count\n",
        "\n",
        "    def increment(self):\n",
        "        \"\"\"Increment the count associated with this node's item.\"\"\"\n",
        "        if self._count is None:\n",
        "            raise ValueError(\"Root nodes have no associated count.\")\n",
        "        self._count += 1\n",
        "\n",
        "    @property\n",
        "    def root(self):\n",
        "        \"\"\"True if this node is the root of a tree; false if otherwise.\"\"\"\n",
        "        return self._item is None and self._count is None\n",
        "\n",
        "    @property\n",
        "    def leaf(self):\n",
        "        \"\"\"True if this node is a leaf in the tree; false if otherwise.\"\"\"\n",
        "        return len(self._children) == 0\n",
        "\n",
        "    @property\n",
        "    def parent(self):\n",
        "        \"\"\"The node's parent\"\"\"\n",
        "        return self._parent\n",
        "\n",
        "    @parent.setter\n",
        "    def parent(self, value):\n",
        "        if value is not None and not isinstance(value, FPNode):\n",
        "            raise TypeError(\"A node must have an FPNode as a parent.\")\n",
        "        if value and value.tree is not self.tree:\n",
        "            raise ValueError(\"Cannot have a parent from another tree.\")\n",
        "        self._parent = value\n",
        "\n",
        "    @property\n",
        "    def neighbor(self):\n",
        "        \"\"\"\n",
        "        The node's neighbor; the one with the same value that is \"to the right\"\n",
        "        of it in the tree.\n",
        "        \"\"\"\n",
        "        return self._neighbor\n",
        "\n",
        "    @neighbor.setter\n",
        "    def neighbor(self, value):\n",
        "        if value is not None and not isinstance(value, FPNode):\n",
        "            raise TypeError(\"A node must have an FPNode as a neighbor.\")\n",
        "        if value and value.tree is not self.tree:\n",
        "            raise ValueError(\"Cannot have a neighbor from another tree.\")\n",
        "        self._neighbor = value\n",
        "\n",
        "    @property\n",
        "    def children(self):\n",
        "        \"\"\"The nodes that are children of this node.\"\"\"\n",
        "        return tuple(self._children.itervalues())\n",
        "\n",
        "    def inspect(self, depth=0):\n",
        "        print ('  ' * depth) + repr(self)\n",
        "        for child in self.children:\n",
        "            child.inspect(depth + 1)\n",
        "\n",
        "    def __repr__(self):\n",
        "        if self.root:\n",
        "            return \"<%s (root)>\" % type(self).__name__\n",
        "        return \"<%s %r (%r)>\" % (type(self).__name__, self.item, self.count)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    from optparse import OptionParser\n",
        "    import csv\n",
        "\n",
        "    p = OptionParser(usage='%prog data_file')\n",
        "    p.add_option('-s', '--minimum-support', dest='minsup', type='int',\n",
        "        help='Minimum allitems support (default: 2)')\n",
        "    p.add_option('-n', '--numeric', dest='numeric', action='store_true',\n",
        "        help='Convert the values in datasets to numerals (default: false)')\n",
        "    p.set_defaults(minsup=2)\n",
        "    p.set_defaults(numeric=False)\n",
        "\n",
        "    options, args = p.parse_args()\n",
        "    if len(args) < 1:\n",
        "        p.error('must provide the path to a CSV file to read')\n",
        "\n",
        "    transactions = []\n",
        "    with open(args[0]) as database:\n",
        "        for row in csv.reader(database):\n",
        "            if options.numeric:\n",
        "                transaction = []\n",
        "                for item in row:\n",
        "                    transaction.append(long(item))\n",
        "                transactions.append(transaction)\n",
        "            else:\n",
        "                transactions.append(row)\n",
        "\n",
        "    result = []\n",
        "    for allitems, support in find_frequent_itemsets(transactions, options.minsup, True):\n",
        "        result.append((allitems,support))\n",
        "\n",
        "    result = sorted(result, key=lambda i: i[0])\n",
        "    for allitems, support in result:\n",
        "        print str(allitems) + ' ' + str(support)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}